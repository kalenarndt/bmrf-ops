---
version: "3"

x-task-vars: &task-vars
  cluster: "{{.cluster}}"
  node: "{{.node}}"
  ceph_disk: "{{.ceph_disk}}"
  ts: "{{.ts}}"
  jobName: "{{.jobName}}"

vars:
  waitForJobScript: "../_scripts/wait-for-k8s-job.sh"
  ts: '{{now | date "150405"}}'

tasks:
  password:
    desc: Gets the password for the rook dashboard
    silent: true
    cmds:
     - kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
  patch-crds:
    desc: Patch Ceph CRDs for
    cmds:
    - |
      for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do
        kubectl get -n rook-ceph "$CRD" -o name | \
        xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{"metadata":{"finalizers": []}}'
      done

  wipe-all:
    desc: Trigger a wipe of Rook-Ceph data on all nodes
    cmds:
      - task: wipe-talos03
      - task: wipe-talos02
      - task: wipe-talos01


  wipe-node:
    desc: Trigger a wipe of Rook-Ceph data on node
    cmds:
      - task: wipe-disk
        vars:
          node: "{{.node}}"
          ceph_disk: "{{.ceph_disk}}"
      - task: wipe-data
        vars:
          node: "{{.node}}"
    vars:
      node: '{{ or .node (fail "`node` is required") }}'
      ceph_disk: '{{ or .ceph_disk (fail "`ceph_disk` is required") }}'

  wipe-talos03:
    desc: Trigger a wipe of Rook-Ceph data on node "talos03"
    cmds:
      - task: wipe-disk
        vars:
          node: "{{.node}}"
          ceph_disk: "/dev/sda"
      - task: wipe-data
        vars:
          node: "{{.node}}"
    vars:
      node: talos03

  wipe-talos02:
    desc: Trigger a wipe of Rook-Ceph data on node "talos02"
    cmds:
      - task: wipe-disk
        vars:
          node: "{{.node}}"
          ceph_disk: "/dev/sda"
      - task: wipe-data
        vars:
          node: "{{.node}}"
    vars:
      node: talos02

  wipe-talos01:
    desc: Trigger a wipe of Rook-Ceph data on node "talos01"
    cmds:
      - task: wipe-disk
        vars:
          node: "{{.node}}"
          ceph_disk: "/dev/sda"
      - task: wipe-data
        vars:
          node: "{{.node}}"
    vars:
      node: talos01

  wipe-disk:
    desc: Wipe all remnants of rook-ceph from a given disk (ex. task rook:wipe-disk node=delta ceph_disk="/dev/nvme0n1")
    silent: true
    internal: true
    cmds:
      - envsubst < <(cat {{.wipeRookDiskJobTemplate}}) | kubectl --context {{.cluster}} apply -f -
      - bash {{.waitForJobScript}} {{.jobName}} default {{.cluster}}
      - kubectl --context {{.cluster}} -n default wait job/{{.jobName}} --for condition=complete --timeout=3m
      - kubectl --context {{.cluster}} -n default delete job {{.jobName}}
    vars:
      cluster: '{{ or .cluster (fail "`cluster` is required") }}'
      node: '{{ or .node (fail "`node` is required") }}'
      ceph_disk: '{{ or .ceph_disk (fail "`ceph_disk` is required") }}'
      jobName: 'wipe-disk-{{- .node -}}-{{- .ceph_disk | replace "/" "-" -}}-{{- .ts -}}'
      wipeRookDiskJobTemplate: "WipeDiskJob.tmpl.yaml"
    env: *task-vars
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: test -f {{.wipeRookDiskJobTemplate}}

  wipe-data:
    desc: Wipe all remnants of rook-ceph from a given disk (ex. task rook:wipe-data node=delta)
    silent: true
    internal: false
    cmds:
      - envsubst < <(cat {{.wipeRookDataJobTemplate}}) | kubectl --context {{.cluster}} apply -f -
      - bash {{.waitForJobScript}} {{.jobName}} default {{.cluster}}
      - kubectl --context {{.cluster}} -n default wait job/{{.jobName}} --for condition=complete --timeout=3m
      - kubectl --context {{.cluster}} -n default delete job {{.jobName}}
    vars:
      cluster: '{{ or .cluster (fail "`cluster` is required") }}'
      node: '{{ or .node (fail "`node` is required") }}'
      jobName: "wipe-rook-data-{{- .node -}}-{{- .ts -}}"
      wipeRookDataJobTemplate: "WipeRookDataJob.tmpl.yaml"
    env: *task-vars
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: test -f {{.wipeRookDataJobTemplate}}
